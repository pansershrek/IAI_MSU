{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9583677,"sourceType":"datasetVersion","datasetId":5844045}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import transformers\nimport torch\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:01.731803Z","iopub.execute_input":"2024-10-29T07:41:01.732396Z","iopub.status.idle":"2024-10-29T07:41:02.661919Z","shell.execute_reply.started":"2024-10-29T07:41:01.732353Z","shell.execute_reply":"2024-10-29T07:41:02.660684Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6ebbe16c3a464e96947b40997ec3c1"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndirectory = '/kaggle/input/semeval-2025-task-3-mu-shroom-dataset/Dataset/Validation'\nall_logits = []\nall_masks = []\n\n# Проход по всем .jsonl файлам в директории\nfor filename in os.listdir(directory):\n    if filename.endswith('.jsonl'):\n        filepath = os.path.join(directory, filename)\n        \n        # Открываем jsonl файл для чтения\n        with open(filepath, 'r', encoding='utf-8') as file:\n            for line in file:\n                # Преобразуем строку в json объект\n                data = json.loads(line.strip())\n                \n                logits = data['model_output_logits']\n                tokens = data['model_output_tokens']\n                hard_labels = data['hard_labels']\n                \n                # Создаем пустую маску длиной по числу токенов\n                mask = [0] * len(tokens)\n                \n                # Отслеживаем позиции символов для каждого токена\n                char_idx = 0\n                token_ranges = []\n                for token in tokens:\n                    token_start = char_idx\n                    token_end = char_idx + len(token)\n                    token_ranges.append((token_start, token_end))\n                    char_idx = token_end\n\n                # Проставляем метки в маске для токенов, попадающих в диапазоны hard_labels\n                for label_range in hard_labels:\n                    label_start, label_end = label_range\n                    for i, (token_start, token_end) in enumerate(token_ranges):\n                        if token_start < label_end and token_end > label_start:\n                            mask[i] = 1\n\n                # Преобразуем логиты и маску в тензоры\n                logits_tensor = torch.tensor(logits).unsqueeze(1)\n                mask_tensor = torch.tensor(mask, dtype=torch.float32)\n\n                # Добавляем в общий список\n                all_logits.append(logits_tensor)\n                all_masks.append(mask_tensor)\n\n\nprint(f\"Total samples collected: {len(all_logits)}\")\n\n# Создание кастомного датасета\nclass LogitsDataset(Dataset):\n    def __init__(self, logits_list, masks_list):\n        self.logits_list = logits_list\n        self.masks_list = masks_list\n\n    def __len__(self):\n        return len(self.logits_list)\n\n    def __getitem__(self, idx):\n        return self.logits_list[idx], self.masks_list[idx]\n\n# Инициализация датасета\ndataset = LogitsDataset(all_logits, all_masks)\n\n# Разделение на тренировочный и тестовый наборы данных (90% и 10%)\ntrain_size = int(0.9 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:13:25.091471Z","iopub.execute_input":"2024-11-12T20:13:25.091993Z","iopub.status.idle":"2024-11-12T20:13:29.278744Z","shell.execute_reply.started":"2024-11-12T20:13:25.091950Z","shell.execute_reply":"2024-11-12T20:13:29.276767Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total samples collected: 499\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Определение модели LSTM\nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(LSTMClassifier, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        out = self.fc(lstm_out)\n        return torch.sigmoid(out).squeeze(-1)  # Сжимаем, чтобы получить (batch, seq_len)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:13:29.281452Z","iopub.execute_input":"2024-11-12T20:13:29.281973Z","iopub.status.idle":"2024-11-12T20:13:29.291211Z","shell.execute_reply.started":"2024-11-12T20:13:29.281931Z","shell.execute_reply":"2024-11-12T20:13:29.289588Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Функция для расчета IoU между предсказанной и истинной масками\ndef calculate_iou(pred, target):\n    #print(pred)\n    #print(target)\n    pred = (pred > 0.5).float()  # Бинаризация предсказаний\n    intersection = (pred * target).sum(dim=1)  # Пересечение\n    union = (pred + target).clamp(0, 1).sum(dim=1)  # Объединение\n    #print(intersection)\n    #print(union)\n    #raise ZeroDivisionError\n    # Устанавливаем IoU в 1 для случаев, когда union равно 0\n    iou = torch.where(union == 0, torch.tensor(1.0, device=union.device), intersection / union)\n    return iou.mean().item()  # Среднее IoU для батча\n\n\n# Функция для оценки на обучающей или тестовой выборках\ndef evaluate_model_with_iou(model, loader):\n    model.eval()\n    total_loss = 0\n    total_iou = 0\n    with torch.no_grad():\n        for logits_batch, mask_batch in loader:\n            outputs = model(logits_batch.to(device))\n            loss = criterion(outputs.to(device), mask_batch.to(device))\n            total_loss += loss.item()\n            total_iou += calculate_iou(outputs, mask_batch.to(device))\n    \n    avg_loss = total_loss / len(loader)\n    avg_iou = total_iou / len(loader)\n    return avg_loss, avg_iou\n\n# Определение модели LSTM\nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(LSTMClassifier, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        out = self.fc(lstm_out)\n        return torch.sigmoid(out).squeeze(-1)  # Сжимаем, чтобы получить (batch, seq_len)\n\n    \n# Создание загрузчиков данных\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=lambda x: (\n    torch.nn.utils.rnn.pad_sequence([item[0] for item in x], batch_first=True),\n    torch.nn.utils.rnn.pad_sequence([item[1] for item in x], batch_first=True)\n))\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=lambda x: (\n    torch.nn.utils.rnn.pad_sequence([item[0] for item in x], batch_first=True),\n    torch.nn.utils.rnn.pad_sequence([item[1] for item in x], batch_first=True)\n))\n\n# Параметры модели\ninput_size = 1\nhidden_size = 2048\noutput_size = 1\nmodel = LSTMClassifier(input_size, hidden_size, output_size).to(device)\n\nmodel = LSTMClassifier(input_size, hidden_size, output_size)  # или любая другая модель\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters in the model: {total_params}\")\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable parameters in the model: {trainable_params}\")\n\n\n\n# Определение функции потерь и оптимизатора\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\nimport matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\n\n# Списки для хранения метрик на каждой эпохе\ntrain_losses = []\ntrain_ious = []\ntest_losses = []\ntest_ious = []\n\n# Обучение модели и оценка на IoU\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    for logits_batch, mask_batch in train_loader:\n        outputs = model(logits_batch.to(device))\n        loss = criterion(outputs.to(device), mask_batch.to(device))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Оценка на обучающей выборке\n    train_loss, train_iou = evaluate_model_with_iou(model, train_loader)\n    # Оценка на тестовой выборке\n    test_loss, test_iou = evaluate_model_with_iou(model, test_loader)\n\n    # Сохраняем метрики\n    train_losses.append(train_loss)\n    train_ious.append(train_iou)\n    test_losses.append(test_loss)\n    test_ious.append(test_iou)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n          f\"Train Loss: {train_loss:.4f}, Train IoU: {train_iou:.4f}, \"\n          f\"Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}\")\n\n# Построение и сохранение графика Loss\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, num_epochs + 1), train_losses, 'b-', label='Train Loss')\nplt.plot(range(1, num_epochs + 1), test_losses, 'r-', label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Test Loss')\nplt.legend()\nplt.grid(True)  # Добавляем сетку\nplt.savefig(\"loss_plot.pdf\")  # Сохраняем график в формате PDF\nplt.close()\n\n# Построение и сохранение графика IoU\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, num_epochs + 1), train_ious, 'b-', label='Train IoU')\nplt.plot(range(1, num_epochs + 1), test_ious, 'r-', label='Test IoU')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.title('Training and Test IoU')\nplt.legend()\nplt.grid(True)  # Добавляем сетку\nplt.savefig(\"iou_plot.pdf\")  # Сохраняем график в формате PDF\nplt.close()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:14:10.769418Z","iopub.execute_input":"2024-11-12T20:14:10.769933Z","iopub.status.idle":"2024-11-12T20:14:37.323496Z","shell.execute_reply.started":"2024-11-12T20:14:10.769886Z","shell.execute_reply":"2024-11-12T20:14:37.321881Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total parameters in the model: 16803841\nTrainable parameters in the model: 16803841\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logits_batch, mask_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 89\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mto(device), mask_batch\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     92\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 40\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(out)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    921\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:10:32.542342Z","iopub.status.idle":"2024-11-12T15:10:32.542686Z","shell.execute_reply.started":"2024-11-12T15:10:32.542514Z","shell.execute_reply":"2024-11-12T15:10:32.542532Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\ndata_pred = []\n# Открываем jsonl файл для чтения\nwith open('/kaggle/input/semeval-2025-task-3-mu-shroom-dataset/Dataset/Validation/mushroom.en-val.v2.jsonl', 'r', encoding='utf-8') as file:\n    for i, line in enumerate(file):\n        item = json.loads(line)\n        print(model(torch.tensor(item['model_output_logits'])))\n        item['soft_labels'] = [{'start': y_pred[i][j][0], 'prob': 1, 'end': y_pred[i][j][1]}\n                               for j in range(len(y_pred[i]))]\n        item['hard_labels'] = [y_pred[i][j] for j in range(len(y_pred[i]))]\n        data_pred.append(item)\n        \ndata_pred","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:10:32.544031Z","iopub.status.idle":"2024-11-12T15:10:32.544865Z","shell.execute_reply.started":"2024-11-12T15:10:32.544589Z","shell.execute_reply":"2024-11-12T15:10:32.544616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\ndata_true = []\n# Открываем jsonl файл для чтения\nwith open('/kaggle/input/semeval-2025-task-3-mu-shroom-dataset/Dataset/Validation/mushroom.en-val.v2.jsonl', 'r', encoding='utf-8') as file:\n    for i, line in enumerate(file):\n        data_true.append(json.loads(line))\n        \ndata_true","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.401831Z","iopub.status.idle":"2024-10-29T07:41:06.402471Z","shell.execute_reply.started":"2024-10-29T07:41:06.402138Z","shell.execute_reply":"2024-10-29T07:41:06.402195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ious = []\n\nfor i in range(50):\n    data1 = data_true[i]\n    data2 = data_pred[i]\n    ious.append(score_iou(data1, data2))\nprint(np.mean(ious))","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.404514Z","iopub.status.idle":"2024-10-29T07:41:06.405099Z","shell.execute_reply.started":"2024-10-29T07:41:06.404816Z","shell.execute_reply":"2024-10-29T07:41:06.404846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ious","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.406863Z","iopub.status.idle":"2024-10-29T07:41:06.407260Z","shell.execute_reply.started":"2024-10-29T07:41:06.407074Z","shell.execute_reply":"2024-10-29T07:41:06.407093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import spearmanr\nimport numpy as np\n\nimport argparse as ap\n\ndef recompute_hard_labels(soft_labels):\n    \"\"\"optionally, infer hard labels from the soft labels provided\"\"\"\n    hard_labels = [] \n    prev_end = -1\n    for start, end in (\n        (lbl['start'], lbl['end']) \n        for lbl in sorted(soft_labels, key=lambda span: (span['start'], span['end']))\n        if lbl['prob'] > 0.5\n    ):\n        if start == prev_end:\n            hard_labels[-1][-1] = end\n        else:\n            hard_labels.append([start, end])\n        prev_end = end\n    return hard_labels\n\ndef load_jsonl_file_to_records(filename):\n    \"\"\"read data from a JSONL file and format that as a `pandas.DataFrame`. \n    Performs minor format checks (ensures that soft_labels are present, optionally compute hard_labels on the fly).\"\"\"\n    df = pd.read_json(filename, lines=True)\n    if 'hard_labels' not in df.columns:\n        df['hard_labels'] = df.soft_labels.apply(recompute_hard_labels)\n    # adding an extra column for convenience\n    df['text_len'] = df.model_output_text.apply(len)\n    df = df[['id', 'soft_labels', 'hard_labels', 'text_len']]\n    return df.sort_values('id').to_dict(orient='records')\n\ndef score_iou(ref_dict, pred_dict):\n    \"\"\"computes intersection-over-union between reference and predicted hard labels, for a single datapoint.\n    inputs:\n    - ref_dict: a gold reference datapoint,\n    - pred_dict: a model's prediction\n    returns:\n    the IoU, or 1.0 if neither the reference nor the prediction contain hallucinations\n    \"\"\"\n    # ensure the prediction is correctly matched to its reference\n    assert ref_dict['id'] == pred_dict['id']\n    # convert annotations to sets of indices\n    ref_indices = {idx for span in ref_dict['hard_labels'] for idx in range(*span)}\n    pred_indices = {idx for span in pred_dict['hard_labels'] for idx in range(*span)}\n    # avoid division by zero\n    if not pred_indices and not ref_indices: return 1.\n    # otherwise compute & return IoU\n    return len(ref_indices & pred_indices) / len(ref_indices | pred_indices)\n\ndef score_cor(ref_dict, pred_dict):\n    \"\"\"computes Spearman correlation between predicted and reference soft labels, for a single datapoint.\n    inputs:\n    - ref_dict: a gold reference datapoint,\n    - pred_dict: a model's prediction\n    returns:\n    the Spearman correlation, or a binarized exact match (0.0 or 1.0) if the reference or prediction contains no variation\n    \"\"\"\n    # ensure the prediction is correctly matched to its reference\n    assert ref_dict['id'] == pred_dict['id']\n    # convert annotations to vectors of observations\n    ref_vec = [0.] * ref_dict['text_len']\n    pred_vec = [0.] * ref_dict['text_len']\n    for span in ref_dict['soft_labels']:\n        for idx in range(span['start'], span['end']):\n            ref_vec[idx] = span['prob']\n    for span in pred_dict['soft_labels']:\n        for idx in range(span['start'], span['end']):\n            pred_vec[idx] = span['prob']\n    # constant series (i.e., no hallucination) => cor is undef\n    if len({round(flt, 8) for flt in pred_vec}) == 1 or len({round(flt, 8) for flt in ref_vec}) == 1 : \n        return float(len({round(flt, 8) for flt in ref_vec}) == len({round(flt, 8) for flt in pred_vec}))\n    # otherwise compute Spearman's rho\n    return spearmanr(ref_vec, pred_vec).correlation\n\ndef main(ref_dicts, pred_dicts, output_file=None):\n    assert len(ref_dicts) == len(pred_dicts)\n    ious = np.array([score_iou(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n    cors = np.array([score_cor(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n    if output_file is not None:\n        with open(output_file, 'w') as ostr:\n            print(f'IoU: {ious.mean():.8f}', file=ostr)\n            print(f'Cor: {cors.mean():.8f}', file=ostr)\n    return ious, cors\n\nif False:\n    p = ap.ArgumentParser()\n    p.add_argument('ref_file', type=load_jsonl_file_to_records)\n    p.add_argument('pred_file', type=load_jsonl_file_to_records)\n    p.add_argument('output_file', type=str)\n    a = p.parse_args()\n    print(a)\n    _ = main(a.ref_file, a.pred_file, a.output_file)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.408411Z","iopub.status.idle":"2024-10-29T07:41:06.408853Z","shell.execute_reply.started":"2024-10-29T07:41:06.408654Z","shell.execute_reply":"2024-10-29T07:41:06.408675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a fact-checking assistant. Your task is to identify fragments of the response that are hallucinations – parts of the text that are factually incorrect or made up by model. Pay attention to dates, numbers, places. Detect only hallucination words, without neighbour words. Give me only a list of fragments-hallucinations you found in model output.\"},\n    {\"role\": \"user\", \"content\": \"\"\"\n    \"query\":\"How many genera does the Erysiphales order contain?\"\\n\"model_output_text\":\"The Elysiphale order contains 5 genera.\"\n    \"\"\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    temperature=0.1\n)\nprint(outputs[0][\"generated_text\"][-1])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.411477Z","iopub.status.idle":"2024-10-29T07:41:06.411934Z","shell.execute_reply.started":"2024-10-29T07:41:06.411723Z","shell.execute_reply":"2024-10-29T07:41:06.411745Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a fact-checking assistant. Your task is to identify fragments of the response that are hallucinations – parts of the text that are factually incorrect or made up by the model. Detect only hallucination words, without neighbour words. Give me only a list of fragments-hallucinations you found in 'model_output'.\"},\n    {\"role\": \"user\", \"content\": \"\"\"\n    \"query\":\"When did Chance the Rapper debut?\"\\n\"model_output_text\":\"Chance the rapper debuted in 2011.\"\n    \"\"\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    temperature=0.1\n)\nprint(outputs[0][\"generated_text\"][-1])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.413332Z","iopub.status.idle":"2024-10-29T07:41:06.413770Z","shell.execute_reply.started":"2024-10-29T07:41:06.413539Z","shell.execute_reply":"2024-10-29T07:41:06.413560Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a fact-checking assistant. Your task is to identify fragments of the response that are hallucinations – parts of the text that are factually incorrect or made up by the model. Detect only hallucination words, without neighbour words. Give me only a list of fragments-hallucinations you found in 'model_output'.\"},\n    {\"role\": \"user\", \"content\": \"\"\"\n    \"query\":\"If today is 14th October, and it is not a leap year, how many days remain until the end of the year?\"\\n\"model_output_text\":\"There are 365 days in a year. So, if today's date is October 13th, then there are only 162 days left until December 31st.\"\n    \"\"\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.415251Z","iopub.status.idle":"2024-10-29T07:41:06.415708Z","shell.execute_reply.started":"2024-10-29T07:41:06.415470Z","shell.execute_reply":"2024-10-29T07:41:06.415489Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a fact-checking assistant. Your task is to identify fragments of the response that are hallucinations – parts of the text that are factually incorrect or made up by the model. Detect only hallucination words, without neighbour words. Give me only a list of fragments-hallucinations you found in 'model_output'.\"},\n    {\"role\": \"user\", \"content\": \"\"\"\n    \"query\":\"Which network released the TV series of the The Punisher?\"\\n\"model_output_text\":\"The Puncher network that released this TV show is Netflix.\"\n    \"\"\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.417020Z","iopub.status.idle":"2024-10-29T07:41:06.417402Z","shell.execute_reply.started":"2024-10-29T07:41:06.417211Z","shell.execute_reply":"2024-10-29T07:41:06.417229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a fact-checking assistant. Your task is to identify fragments of the response that are hallucinations – parts of the text that are factually incorrect or made up by model. Pay attention to dates, numbers, places. Detect only hallucination words, without neighbour words. Give me only a list of fragments-hallucinations you found in model output.\"},\n    {\"role\": \"user\", \"content\": \"\"\"\n    \"query\":\"When was Bischofsheim a constitutent community of Mainz?\"\\n\"model_output_text\":\" Bishofshain was a constituent community in the early Middle Ages.\\n\"\n    \"\"\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    temperature=0.1\n)\nprint(outputs[0][\"generated_text\"][-1])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.418516Z","iopub.status.idle":"2024-10-29T07:41:06.418980Z","shell.execute_reply.started":"2024-10-29T07:41:06.418774Z","shell.execute_reply":"2024-10-29T07:41:06.418796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a fact-checking assistant. Your task is to identify fragments of the response that are hallucinations – parts of the text that are factually incorrect or made up by the model. Detect only hallucination words, without neighbour words. Give me only a list of fragments-hallucinations you found in 'model_output_text'. Pay attention to the facts, not to literacy and typos.\"},\n    {\"role\": \"user\", \"content\": \"\"\"\n    query:\"Who first described the white-winged chough?\"\\nmodel_output:\"The white winged cough was first discovered by the English naturalist John Latham in 1770.\"\n    \"\"\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    temperature=0.1\n)\nprint(outputs[0][\"generated_text\"][-1])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:41:06.420692Z","iopub.status.idle":"2024-10-29T07:41:06.421089Z","shell.execute_reply.started":"2024-10-29T07:41:06.420906Z","shell.execute_reply":"2024-10-29T07:41:06.420925Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}